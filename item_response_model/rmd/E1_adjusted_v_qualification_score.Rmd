---
title: \vspace{3.5in}"Comparing Adjusted and Original Qualification Score - Experiment-1"
author: "Christian Medeiros Adriano"
date: "`r Sys.Date()`"
bibliography: "..//..//bibtex//rmd_references.bib"
csl: "..//..//bibtex//acm-sig-proceedings.csl"
output:
   pdf_document:
      fig_caption: true
      number_sections: true
---
\newpage
\tableofcontents 
\listoffigures
\listoftables
\newpage 

    
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)

source("C://Users//Christian//Documents//GitHub//CausalModel_FaultUnderstanding//data_loaders//load_consent_create_indexes_E1.R")
df_consent <- load_consent_create_indexes();

```

# About this report

Investigate the impact of the score adjustment on the association between score and the other covariates, e.g, years of experience, age, gender, duration, explanation size, confidence, and accuracy of tasks.

# Comparing Distributions

``` {r align distributions, include=FALSE}
shift <-min(df_consent$adjusted_score) 
if(shift<0){
  irt_scores <- df_consent$adjusted_score + abs(shift)
} else if(shift>0){
    irt_score <- df_consent$adjusted_score - abs(shift)
}
```


## How do the distribution overlap?

```{r density plots, echo=FALSE}

#Center (subtract the mean) and Scales (divided by the standard deviation)
#qualification_scores <- scale(df_consent$qualification_score, center=TRUE, scale=TRUE)
#irt_scores <- scale(df_consent$adjusted_score, center=TRUE, scale=TRUE)

score_type <- rep("original",length(df_consent$qualification_score))
original_score_list <- cbind(df_consent$qualification_score,score_type)
score_type <- rep("adjusted",length(irt_scores))
irt_score_list <- cbind(irt_scores,score_type)

all_score_list <- rbind(original_score_list,irt_score_list)
df_all_scores <- data.frame(all_score_list)
colnames(df_all_scores) <- c("score","type")
df_all_scores$score <- as.numeric(as.character(df_all_scores$score))

df_all_scores %>%
  ggplot(aes(x=score, fill=type)) +
  #geom_histogram(binwidth=0.05, color="darkgrey", fill="lightblue") +
  geom_density(alpha=0.3)+
  theme_minimal()+
  theme(
    legend.position=c(0.85, 0.90),
    panel.spacing = unit(0.1, "lines"),
    strip.text.x = element_text(size = 12),
    plot.title = element_text(size=14),
    axis.text.x = element_text(angle = 20, hjust = 1, size=12)
  ) +
  xlab("Scores") +
  ylab("Frequency") +
  ggtitle("Qualification score distribution E1") 

```

The chart shows that the adjusted score smoothed the distribution,
but still preserved the two general patterns of concentration on 
high and low medium-to-low scores.

However, it also increased the frequency of the lowest score and
highest score groups. The final distribution has more oa an
exponential shape, while the original one had a right skewed Gaussian
shape.


The reason for the smoothing is two-fold: adjusted score is continuous scale and the original shifted some very low scores to a low-to-medium score. The latter
corresponds to giving a lower weight to questions that most people
got it correctly This was the case of question 4.

## Are the distributions statistically significant distinct?
In order to pick the appropriate statistical test, I tested if the distributions are normal. For that I used the Shapiro-Wilk normality test (@royston1995shapiro). The null hypothesis of Shapiro's test is that the population is distributed normally.


```{r hypotheses tests, echo=TRUE}
shapiro.test(scale(df_consent$qualification_score))
shapiro.test(scale(df_consent$adjusted_score))
```
The two distributions rejected the null hypothesis of the Shapiro normality test, which implies that the distributions are not normal.

Therefore, I chose the non-parametric Kruskal-Wallis test @kruskalwallis1952ranktest with a null-hypothesis that the mean values of the original and adjusted score are the same.

Because the distributions are in different scale, I rescale them so they are still comparable. The rescaling involved (1) centering on their means and (2) normalizing. This corresponds respectively to (1) subtract the mean from each data point and (2) divide each data point by the standard deviation.


```{r hypotheses tests, echo=TRUE}
kruskal.test(scale(df_consent$qualification_score,center=TRUE,scale=TRUE),
             scale(df_consent$adjusted_score,center=TRUE,scale=TRUE)
             )
```

The two distributions are statistically distinct. 

## How does the entropy (spread) of the distributions compare?


# Comparing Rankings

I used to metrics to compare the models: ambiguity and discrepancy.

Ambiguity: How many participants are assigned conflicting scores in the two competing models (original and adjusted model)?

Discrepancy: What is the maximum number of predictions
that could change if we were to switch the model that we
deploy with a competing model?

## Ranking the participants by their score, how many changed their positions? 

## How do participants who increased their ranking position compare to others w.r.t. their years of experience?

## How do participants who decreased their ranking position compare to others w.r.t. their years of experience?

## Are participants who increased, descreased, not changed ranking positions statistically significant distinct w.r.t. to score and years of experience?

# Comparing Correlation
## Years of programming and Score 
## Age and Score 
## Gender and Score

# References

